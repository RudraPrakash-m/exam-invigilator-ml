# ğŸ“ Smart Exam Invigilator System

**Pose-Based AI Cheating Detection**

---

## ğŸ“Œ Project Description

The **Smart Exam Invigilator System** is an AI-based surveillance solution designed to monitor examination halls and detect **suspicious student behavior** in real time using **pose estimation and machine learning**.

The system analyzes **head movement, hand movement, and body posture** of students over time and classifies behavior as **Normal** or **Suspicious** without using face recognition, ensuring privacy.

---

## ğŸ§  Core Idea

Instead of detecting faces or identities, the system:

* Detects **people**
* Extracts **pose keypoints**
* Tracks **motion patterns across frames**
* Classifies behavior using a trained ML model
* Assigns each student to a **fixed seat zone (A1, A2, â€¦)**

---

## ğŸ§© Technologies Used

| Component            | Technology                |
| -------------------- | ------------------------- |
| Video Processing     | OpenCV                    |
| Pose Detection       | YOLOv8 Pose               |
| Machine Learning     | XGBoost                   |
| Programming Language | Python                    |
| Data Storage         | CSV                       |
| Camera Support       | Laptop Webcam / MP4 Video |

---

## ğŸ“ Project Structure (From ZIP)

```
exam_invigilator_1/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ 1_extract_keypoints.py     # Extract pose keypoints from video
â”‚   â”œâ”€â”€ 2_build_features.py        # Build temporal features
â”‚   â”œâ”€â”€ 3_train_model.py           # Train ML model
â”‚   â””â”€â”€ 4_live_detection.py        # Real-time detection (webcam / video)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ videos/
â”‚   â”‚   â””â”€â”€ train_video.mp4        # Training / testing video
â”‚   â””â”€â”€ window_features.csv        # Extracted feature dataset
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ cheating_model.json        # Trained XGBoost model
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ events.csv                 # Detection logs
â”‚
â”œâ”€â”€ snapshots/
â”‚   â””â”€â”€ *.jpg                      # Evidence snapshots
â”‚
â”œâ”€â”€ yolo11s-pose.pt                # YOLO Pose model
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”„ Complete Pipeline

```
MP4 / Camera Input
        â†“
YOLO Pose Detection
        â†“
Pose Keypoints (17 body points)
        â†“
Temporal Feature Extraction (Windowed)
        â†“
XGBoost Classifier
        â†“
Suspicious / Normal Decision
        â†“
Zone-Based Label (A1, A2)
        â†“
Logging + Snapshots
```

---

## ğŸ§ Pose Keypoints Used

The system uses YOLOâ€™s **COCO 17-keypoint format**:

| Feature          | Keypoints        |
| ---------------- | ---------------- |
| Head movement    | Nose (0)         |
| Hand movement    | Wrists (9, 10)   |
| Body orientation | Shoulders (5, 6) |

These keypoints are analyzed over multiple frames to detect meaningful behavior.

---

## ğŸªŸ Sliding Window & Cooldown

* **Sliding Window (30 frames)**
  Ensures decisions are based on motion over time, not single frames.

* **Cooldown Mechanism**
  Prevents repeated alerts/logs for the same student within a short time window.

This keeps the system **stable and realistic**.

---

## ğŸª‘ Zone-Based Identification

Each student is assigned a **seat zone**:

```
A1   A2
```

### Why zone-based IDs?

* Exam seating is fixed
* No tracker ID flickering
* Easy for invigilators to understand
* No personal identity stored

Displayed labels:

```
A1
Suspicious A2
```

---

## ğŸ“Š Output & Evidence

### On Screen

* ğŸŸ¢ Green box â†’ Normal
* ğŸ”´ Red box â†’ Suspicious
* Label â†’ Zone ID

### Logs (`logs/events.csv`)

```
timestamp, zone, probability, label
```

### Snapshots

* Automatically captured when suspicious activity is detected
* Stored for later review

---

## ğŸ¥ Running the Project

### 1ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

### 2ï¸âƒ£ Step-by-Step Execution

#### Step 1: Extract Keypoints

```bash
python src/1_extract_keypoints.py
```

#### Step 2: Build Features

```bash
python src/2_build_features.py
```

#### Step 3: Train Model

```bash
python src/3_train_model.py
```

#### Step 4: Run Detection (Webcam or Video)

```bash
python src/4_live_detection.py
```

---

## ğŸ¥ Input Modes Supported

### âœ” MP4 Video (Testing)

```python
cap = cv2.VideoCapture("data/videos/train_video.mp4")
```

Used for:

* Training
* Debugging
* Evaluation

### âœ” Laptop Webcam (Live)

```python
cap = cv2.VideoCapture(1, cv2.CAP_DSHOW)
```

Used for:

* Real-time monitoring
* Live demo

---

## ğŸ” Privacy & Ethics

* âŒ No face recognition
* âŒ No identity storage
* âœ” Pose-only analysis
* âœ” GDPR-friendly approach

---

## ğŸš€ Future Scope

* Robot-based invigilator (ESP32)
* Multi-camera fusion
* Audio alerts
* Dashboard monitoring
* Depth-aware detection

---

## ğŸ“ Academic Relevance

This project demonstrates:

* Computer Vision
* Pose Estimation
* Temporal Machine Learning
* Real-world system design
* Ethical AI implementation

---

## ğŸ‘¨â€ğŸ’» Author

**Rudra**
B.Tech â€“ Computer Science Engineering
AI & Smart Surveillance Systems

---

## âœ… Final Note

This project is designed to be **realistic, explainable, and deployable**, not just a demo.
It closely follows how **real AI surveillance systems are engineered**.
